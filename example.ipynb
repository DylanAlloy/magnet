{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üíª set up your environment\n",
    "\n",
    "import `Processor` to restructure and clean your data in a way which allows for rigorous, accurate tuning according to your goals\n",
    "\n",
    "###### ‚ÑπÔ∏è your source data must be csv/json/parquet path OR a dataframe object and contain **at least** one column with plaintext\n",
    "\n",
    "###### ‚ö°Ô∏èüß≤ assuming for each transformation that you have the step-wise data ready to go, this block is all you need to initialize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from magnet.filings import Processor\n",
    "source_data_file = \"./raw/kb_export_clean.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìë create sentences from plaintext\n",
    "\n",
    "we set an an input file with a `Processor` class to create `filings` out of our data\n",
    "\n",
    "###### ‚ÑπÔ∏è you do not need an `id` column, we will make a document-level, integer-wise one for each of your sentences automatically, but keep this in mind for re-indexing your embeddings back to sentences or documents!\n",
    "\n",
    "###### ‚ÑπÔ∏è then we load the specific raw data file into memory! this is to protect your sequential workflow requirements but if you need to conserve resources during steps, look into the `.unload()` function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filings = Processor()\n",
    "filings.load(source_data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ü•≥ great! let's process our data, _fast_\n",
    "\n",
    "‚ö°Ô∏èüß≤ first we extract sentences for our embedding model to get initial scores and examples which we call `filings`\n",
    "\n",
    "###### ‚ÑπÔ∏è don't forget to declare your plaintext column's name! we do not persist this between objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filings.export_as_sentences('./data/sentences.parquet','clean','id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üßÆ indexing data \n",
    "\n",
    "in `magnet.ize`, we have different submodules responsible for different parts of building our \"data field\" of vectors.\n",
    "\n",
    "import `charge` to create a \"Pole\" and index your sentences to it using your embedding model.\n",
    "you can think of the vectors as electrons which belong to charged particles in a magnetic pole, or you can ignore the metaphor completely!\n",
    "\n",
    "then we save our embeddings to re-use later and upload/share."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from magnet.ize import charge\n",
    "import pandas as pd\n",
    "\n",
    "sentences = pd.read_parquet('./data/sentences.parquet')\n",
    "charge = charge.Pole()\n",
    "charge.index_document_embeddings(df=sentences)\n",
    "charge.save_embeddings('./data/sentence_embeddings.index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ü§Ø amazing and easy! \n",
    "\n",
    "let's now use these scores to create positive and negative training examples. here we search, re-index back to our sentences in plaintext with their IDs, and return the results as positive and negative examples\n",
    "\n",
    "in `magnet.ron`, we have functions to create tuned versions of your `Pole` after charging with sentences from your knowledge base. You can think of this as energy interacting with a prism and orienting certain features in certain angles (you'll understand this more as we dive into embedding model use-cases!). In our example we will do positively and negatively correlated samples for finetuning on a `similarity` task from our vast volume of data (energy).\n",
    "\n",
    "###### ‚ÑπÔ∏è we use a random distribution the size of your `split` argument from your data! you can also change the amount of your index is allowed to pass through your prism with `k` argument\n",
    "\n",
    "Let's create our `Prism` class from `tune`\n",
    "\n",
    "###### ‚ÑπÔ∏è check out a given model's documentation for more on how to format your data, ours defaults to the format required to tune `BAAI/bge-large-en-v1.5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from magnet.ron import tune\n",
    "import pandas as pd\n",
    "\n",
    "sentences = pd.read_parquet('./data/sentences.parquet')\n",
    "task = 'similarity'\n",
    "kb_prepper = tune.Prism()\n",
    "kb_prepper.load(sentences)\n",
    "kb_prepper.generate_training_data(out_dir='./data/', index='./data/sentence_embeddings.index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üéà‚òÅÔ∏è upload to S3 on AWS\n",
    "\n",
    "how easy! not much to say here other than this will upload your entire processed data folder when done (we assume you got your cleaned data from somewhere!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from magnet.utils import Utils\n",
    "\n",
    "Utils().upload_to_s3(\n",
    "    './data/'\n",
    "    , ('AWS_CLIENT_KEY', 'AWS_SECRET_KEY')\n",
    "    , 'bucket_name'\n",
    "    , 'finetuning_data'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
