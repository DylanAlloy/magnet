{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üíª set up your environment\n",
    "\n",
    "###### your source data should a csv/json path or a dataframe and contain **at least** one column with plaintext\n",
    "\n",
    "###### ‚ö°Ô∏è assuming for each transformation that you have the step-wise data ready to go, this block is all you need to initialize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU being used. Careful, inference might be very slow!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93müö® WARN: CUDA is not available on this machine.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from magnet.utils import Utils, _f\n",
    "\n",
    "raw_dir = \"./raw\"\n",
    "cleaned_dir = \"./data\"\n",
    "source_data_file = \"knowledge_base_export.csv\"\n",
    "plaintext_column = \"clean\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìë create sentences from plaintext\n",
    "###### we set an output filename, an input directory, and an output directory with our `Processor` class\n",
    "\n",
    "###### then we load the specific raw data file into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m‚ÑπÔ∏è INFO: Processor init\u001b[0m\n",
      "\u001b[92müåä SUCCESS: loaded - /Users/dylanalloy/VSCode/LLM/kb-embeddings/lib/magnet.git/raw/knowledge_base_export.csv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from magnet.processing import Processor\n",
    "sentences_filename = 'knowledge_base_sentences'\n",
    "kb_sentence_proc = Processor(sentences_filename, raw_dir, cleaned_dir)\n",
    "kb_sentence_proc.load(source_data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ü•≥ great! let's process our data, _fast_\n",
    "\n",
    "##### ‚ö°Ô∏è first we extract sentences for our embedding model to get initial scores and examples from\n",
    "\n",
    "###### don't forget to declare your plaintext column's name!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[96m‚òïÔ∏è WAIT: get coffee or tea - 5726 processing...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5725 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5725/5725 [05:53<00:00, 16.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92müåä SUCCESS: üó≥Ô∏è  - /Users/dylanalloy/VSCode/LLM/kb-embeddings/lib/magnet.git/data/knowledge_base_sentences.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "kb_sentence_proc.export_with_sentences(plaintext_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### üìä now let's score our sentences against those found in random batches of documents!\n",
    "\n",
    "###### üìñ 1Ô∏è‚É£ `split` by default is 16 which uses said fraction of your data to create examples from.\n",
    "\n",
    "###### üìñ 2Ô∏è‚É£ we then create a subsampling of our newly scored data. this is a requirement for sorting positive and negative samples later when we export finetuning datasets. \n",
    "\n",
    "###### we're going to use `FinePrep` class to prepare our data for finetuning training runs\n",
    "\n",
    "###### don't forget to declare your `group_by` column in `generate_scored_data` as well as the name of the original plaintext column so we can persist it across datasets\n",
    "\n",
    "###### **‚ú® this scoring can be done with any `sentence-transformers` model you like, not necessarily the one you are finetuning (`model=''`)! you can also insert a custom** `prompt` **if the model benefits from it ‚ú®**\n",
    "\n",
    "###### (for example, when using `bge-large-en-v1.5` for `retrieval` instead of `similarity` tuning, a prompt is required)\n",
    "\n",
    "###### üö® `generate_scored_data` can take some time if `use_multiprocessing` is not enabled, and using it is compute-intensive\n",
    "\n",
    "###### ‚ÑπÔ∏è multiprocessing is not needed if you are using CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m‚ÑπÔ∏è INFO: FinePrep init\u001b[0m\n",
      "\u001b[92müåä SUCCESS: loaded - /Users/dylanalloy/VSCode/LLM/kb-embeddings/lib/magnet.git/data/knowledge_base_sentences.json\u001b[0m\n",
      "\u001b[96m‚òïÔ∏è WAIT: get coffee or tea - 178 (1/32 of your data) processing...\u001b[0m\n",
      "\u001b[93müö® WARN: 1/10 processes started from index 0 to 17/178 (17)\u001b[0m\n",
      "\u001b[93müö® WARN: 2/10 processes started from index 17 to 34/178 (17)\u001b[0m\n",
      "\u001b[93müö® WARN: 3/10 processes started from index 34 to 51/178 (17)\u001b[0m\n",
      "\u001b[93müö® WARN: 4/10 processes started from index 51 to 68/178 (17)\u001b[0m\n",
      "\u001b[93müö® WARN: 5/10 processes started from index 68 to 85/178 (17)\u001b[0m\n",
      "\u001b[93müö® WARN: 6/10 processes started from index 85 to 102/178 (17)\u001b[0m\n",
      "\u001b[93müö® WARN: 7/10 processes started from index 102 to 119/178 (17)\u001b[0m\n",
      "\u001b[93müö® WARN: 8/10 processes started from index 119 to 136/178 (17)\u001b[0m\n",
      "\u001b[93müö® WARN: 9/10 processes started from index 136 to 153/178 (17)\u001b[0m\n",
      "\u001b[93müö® WARN: 10/10 processes started from index 153 to 178/178 (17)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m üåä SUCCESS: sample 11 - comparing 1023801 üßÆ 1003579\u001b[0m: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [1:32:33<00:00, 326.70s/it]it]\n",
      "\u001b[92m üåä SUCCESS: sample 9 - comparing 1021282 üßÆ 1015704\u001b[0m: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [1:39:15<00:00, 350.32s/it]t]  \n",
      "\u001b[92m üåä SUCCESS: sample 0 - comparing 1024244 üßÆ 1016594\u001b[0m: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [1:57:30<00:00, 414.74s/it] ]\n",
      "\u001b[92m üåä SUCCESS: sample 20 - comparing 1020751 üßÆ 1018459\u001b[0m: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [1:58:24<00:00, 417.91s/it]]\n",
      "\u001b[92m üåä SUCCESS: sample 3 - comparing 1021216 üßÆ 1022778\u001b[0m: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [2:24:14<00:00, 509.08s/it]]]] \n",
      "\u001b[92m üåä SUCCESS: sample 16 - comparing 1016281 üßÆ 1015251\u001b[0m: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [2:34:52<00:00, 546.63s/it] ]\n",
      "\u001b[92m üåä SUCCESS: sample 3 - comparing 1011984 üßÆ 1008179\u001b[0m: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [2:45:50<00:00, 585.30s/it]]   \n",
      "\u001b[92m üåä SUCCESS: sample 12 - comparing 1003460 üßÆ 1016268\u001b[0m: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [2:48:02<00:00, 593.10s/it]\n",
      "\u001b[92m üåä SUCCESS: sample 10 - comparing 1011782 üßÆ 1018454\u001b[0m: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [2:50:16<00:00, 408.67s/it]\n",
      "\u001b[92m üåä SUCCESS: sample 8 - comparing 1018268 üßÆ 1023706\u001b[0m: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 17/17 [3:07:05<00:00, 660.30s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92müåä SUCCESS: saved to - /Users/dylanalloy/VSCode/LLM/kb-embeddings/lib/magnet.git/data/scored_knowledge_base_sentences.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from magnet.finetune import FinePrep\n",
    "\n",
    "sentences_data_file = 'knowledge_base_sentences.json'\n",
    "scored_sentences_filename = 'scored_knowledge_base_sentences'\n",
    "group_by = 'answerId'\n",
    "task = 'similarity'\n",
    "kb_prepper = FinePrep(\n",
    "    scored_sentences_filename\n",
    "    , cleaned_dir\n",
    "    , cleaned_dir\n",
    ")\n",
    "kb_prepper.load(sentences_data_file)\n",
    "kb_prepper.generate_scored_data(\n",
    "    group_by\n",
    "    , plaintext_column\n",
    "    , split=32\n",
    "    , use_multiprocessing=True\n",
    "    , task=task\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ü§Ø amazing and easy! \n",
    "\n",
    "###### let's now use these scores to create positive and negative training examples for `BAAI/bge-large-en-v1.5` finetuning by creating a default 0.7 quantile split\n",
    "\n",
    "###### ‚ÑπÔ∏è check out a given model's documentation for more on the quantitative parts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94m‚ÑπÔ∏è INFO: FinePrep init\u001b[0m\n",
      "\u001b[92müåä SUCCESS: loaded - /Users/dylanalloy/VSCode/LLM/kb-embeddings/lib/magnet.git/data/scored_knowledge_base_sentences.json\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[93m üíô ‚£∑: processed  - \"Dr. Philip N. Jefferson took the oath of office as Vice Chair of the Board of Governors of the Federal Reserve System on Wednesday.\"\u001b[0m: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 75864/75864 [01:08<00:00, 1111.35it/s]                                                                          ]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92müåä SUCCESS: written - /Users/dylanalloy/VSCode/LLM/kb-embeddings/lib/magnet.git/data/finetune_kb_dataset.jsonl\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from magnet.finetune import FinePrep\n",
    "\n",
    "finetuned_data_file = 'finetune_kb_dataset'\n",
    "\n",
    "finetune_prepper = FinePrep(finetuned_data_file, cleaned_dir, cleaned_dir)\n",
    "finetune_prepper.load('scored_knowledge_base_sentences.json')\n",
    "finetune_prepper.generate_training_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"but wait ü´µ, there's more! ü§™\" _('hard negative' mining)_\n",
    "\n",
    "##### using Meta's `faiss` we can create a performant vector index for deriving 'hard negatives' to assist in extracting meaning from the quantile splits, then collate the two datasets for one mega dataset with very good representation without overfitting on our knowledge base!\n",
    "\n",
    "###### üß† **intuition**: mining hard positives with a simple quantile split works to get an edge in our finetuning dataset because positive correlations are relativistically rare in a specific domain by the 80th percentile (`quant` default). less insight comes from negative examples that have relevance scores in the 20th percentile because this can be due to length differences in the samples, low standard deviation in the sample set, etc.\n",
    "\n",
    "###### üìù **takeaway**: mining hard negatives is an efficient way to achieve a similarly robust representation of negatively correlated contents from your knowledge base for training as we do the positively correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93müö® WARN: CUDA is not available on this machine.\u001b[0m\n",
      "\u001b[96m‚òïÔ∏è WAIT: inferencing embeddings for corpus - 2529\u001b[0m\n",
      "\u001b[96m‚òïÔ∏è WAIT: inferencing embedding for queries - 75852\u001b[0m\n",
      "\u001b[92müåä SUCCESS: create index and search\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1186/1186 [00:05<00:00, 202.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92müåä SUCCESS: written - /Users/dylanalloy/VSCode/LLM/kb-embeddings/lib/magnet.git/data/fn_hn_kb.jsonl\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "input_file='finetune_kb_dataset.jsonl'\n",
    "output_file='fn_hn_kb'\n",
    "finetune_prepper.find_knn_neg(\n",
    "    model='BAAI/bge-large-en-v1.5'\n",
    "    , input_file=input_file\n",
    "    , output_file=output_file\n",
    "    , use_gpu=Utils().check_cuda()\n",
    "    , sample_range=[0,500]\n",
    "    , num_hard_negatives=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93müö® WARN: uploading to S3 - ./data/fn_hn_kb.jsonl\u001b[0m\n",
      "\u001b[92müåä SUCCESS: uploaded - bucket_name/finetuning_data/fn_hn_kb.jsonl\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "Utils().upload_to_s3(\n",
    "    './data/fn_hn_kb.jsonl'\n",
    "    , ('AWS_CLIENT_KEY', 'AWS_SECRET_KEY')\n",
    "    , 'bucket_name'\n",
    "    , 'finetuning_data'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
